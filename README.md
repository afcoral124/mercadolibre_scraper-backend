# Proyecto: Automatizaci√≥n de Scraping y Backend para Mercado Libre

Este proyecto implementa un sistema completo de scraping, procesamiento y almacenamiento de informaci√≥n sobre productos en venta en [mercadolibre.com.co](https://www.mercadolibre.com.co/). El sistema incluye:

* Scraper asincr√≥nico con limpieza de datos.
* API REST en FastAPI conectada a una base de datos PostgreSQL en la nube.
* Script de automatizaci√≥n con respaldo y carga incremental.

Se scrapean los siguientes campos de los art√≠culos:

* nombre_articulo (str): Nombre del producto o art√≠culo.
* precio (int): Precio del producto en COP.
* calificacion_promedio (float): Valoraci√≥n promedio del producto.
* antidad_calificaciones (int): N√∫mero de valoraciones que ha recibido.
* descripcion (str): Descripci√≥n textual del producto.
* enlace_articulo (str): URL √∫nica del art√≠culo.

Con estos campos extraidos puede realizarse un posterior an√°lisis de mercado, entender cuales son los precios m√°s competitivos, analizar qu√© productos representan un mejor beneficio en t√©rminos de calidad/precio contrastando las variables de calificaciones con la del precio, monitoreo de cat√°logos, etc.


---

## ü§ù Stack Tecnol√≥gico y Justificaci√≥n

| Tecnolog√≠a              | Rol                       | Justificaci√≥n                                                                |
| ----------------------- | ------------------------- | ---------------------------------------------------------------------------- |
| **Python 3.10+**        | Lenguaje principal        | Sencillo, robusto y con librer√≠as maduras.                                   |
| **FastAPI**             | Backend REST API          | R√°pido, moderno y con validaci√≥n autom√°tica (Pydantic).                      |
| **PostgreSQL (Render)** | Base de datos en la nube  | Robusta, gratuita en Render, soporte JSON.                                   |
| **SQLAlchemy**          | ORM                       | Permite manejar la base de datos como objetos, con flexibilidad y seguridad. |
| **requests / aiohttp**  | HTTP sync/async           | Para scraping y concurrencia.                                                |
| **BeautifulSoup**       | Parser HTML               | Limpieza de contenido web.                                                   |
| **pandas**              | Manejo de datos tabulares | Escritura de CSVs y depuraci√≥n.                                              |
| **argparse**            | CLI                       | Parametrizaci√≥n del script.                                                  |

---

## üöÄ Estructura del Proyecto

```bash
mercadolibre_scraper_backend/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/           # Conexi√≥n y esquema de base de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routers/      # Endpoints FastAPI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py      # Inicializaci√≥n del servidor
‚îú‚îÄ‚îÄ scraping/              # L√≥gica del scraper
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ automation.py   # Script CLI para ejecutar todo el flujo
‚îú‚îÄ‚îÄ backups/               # CSVs generados por scraping
‚îú‚îÄ‚îÄ logs/                  # Registros de ejecuci√≥n
‚îú‚îÄ‚îÄ .env                   # Variables de entorno sensibles
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md
```

---

## üîé Flujo General del Proyecto

1. **Scraper** busca productos seg√∫n un t√©rmino de b√∫squeda y hace scraping asincr√≥nico.
2. **Limpieza de datos** normaliza precios, enlaces, descripciones y calificaciones.
3. **Verificaci√≥n incremental**: se consultan los enlaces ya almacenados en la base de datos para evitar duplicados.
4. **Carga a la API REST**: los registros nuevos se env√≠an al backend en FastAPI.
5. **Backup**: se guarda un CSV local con los resultados scrapeados.
6. **Logs**: se registran errores, resumen del proceso y timestamp.

---

## üèóÔ∏è Diagrama de Arquitectura

```
           +----------------------+     +------------------+
           |   Script CLI (user)  | --> | automation.py     |
           +----------------------+     +--------+---------+
                                                 |
                                                 |
                                         +-------v--------+
                                         | scraping/      |
                                         | scraper.py     |
                                         +-------+--------+
                                                 |
                                       +---------v----------+
                                       | BeautifulSoup +    |
                                       | Aiohttp/Requests   |
                                       +---------+----------+
                                                 |
                                         +-------v--------+
                                         | Limpieza CSV    |
                                         +-------+--------+
                                                 |
                                  +--------------v-------------+
                                  |  FastAPI Backend            |
                                  |  Routers + Schemas         |
                                  +--------------+-------------+
                                                 |
                                  +--------------v-------------+
                                  |   SQLAlchemy ORM Models     |
                                  +--------------+-------------+
                                                 |
                                         +-------v--------+
                                         | PostgreSQL DB  |
                                         | Render Cloud   |
                                         +----------------+
```

---

## üìÉ Tabla de Funciones Clave

| Archivo         | Funci√≥n                           | Prop√≥sito                                                     |
| --------------- | --------------------------------- | ------------------------------------------------------------- |
| `scraper.py`    | `obtener_url_todos_los_articulos` | Paginaci√≥n de resultados y recolecci√≥n de URLs de productos.  |
|                 | `scrapear_lista_articulos_async`  | Scraping asincr√≥nico de cada producto.                        |
|                 | `limpiar_datos_articulos`         | Normalizaci√≥n de precios, enlaces, y validaci√≥n de registros. |
|                 | `guardar_en_csv`                  | Almacenamiento local con timestamp.                           |
| `automation.py` | `main(args)`                      | Orquesta scraping + limpieza + backup + carga.                |
|                 | `obtener_enlaces_existentes`      | Recupera enlaces desde la API con paginaci√≥n.                 |
|                 | `enviar_registro`                 | POST a la API con manejo de errores y duplicados.             |

---

## ‚öñÔ∏è Despliegue y Ejecuci√≥n

### 1. Crear entorno virtual e instalar dependencias

```bash
python -m venv venv
venv\Scripts\activate   # (Windows)
pip install -r requirements.txt
```

### 2. Configurar las variables en `.env`

```env
DB_HOST=your-db-host.render.com
DB_NAME=your-db-name
DB_USER=your-db-user
DB_PASSWORD=your-db-password
DB_PORT=your-db-port
```

### 3. Iniciar la API (desde la carpeta `backend`)

```bash
uvicorn app.main:app --reload
```

### 4. Ejecutar el scraper con CLI o cron

Con CLI:
```bash
python scripts/automation.py --articulo "laptop hp" --paginas 3 --guardar_csv --concurrencia 50
```

Con Cron (Por ejemplo para ejecutar todos los d√≠as a las 09:00 AM):
```bash
crontab -e

0 9 * * * /usr/bin/python3 /ruta/completa/al/proyecto/scripts/automation.py --articulo "laptop hp" --paginas 3 --guardar_csv --concurrencia 50 >> /ruta/completa/al/proyecto/logs/cron.log 2>&1
```

---

## üé• Video de Demostraci√≥n

Video del Scraper en ejecuci√≥n:
https://youtu.be/u4GSGzfMa2g?si=OV6IExebIwkjr5jG

Video del Script de automatizaci√≥n + API Rest & Endpoints + Base de datos desplegada en la nube
https://youtu.be/ZLPAyebJn34?si=BdysK0i7pAR1ByQ5

---

## üîñ Recomendaciones Finales

* El sistema est√° preparado para correr diariamente como tarea programada (cron).
* Se pueden ampliar los campos de scraping seg√∫n la categor√≠a del producto.
* Ideal para monitorear precios, disponibilidad o cambios en cat√°logos.

---


