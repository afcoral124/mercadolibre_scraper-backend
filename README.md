# Proyecto: AutomatizaciÃ³n de Scraping y Backend para Mercado Libre

Este proyecto implementa un sistema completo de scraping, procesamiento y almacenamiento de informaciÃ³n sobre productos en venta en [mercadolibre.com.co](https://www.mercadolibre.com.co/). El sistema incluye:

* Scraper asincrÃ³nico con limpieza de datos.
* API REST en FastAPI conectada a una base de datos PostgreSQL en la nube.
* Script de automatizaciÃ³n con respaldo y carga incremental.

Se scrapean los siguientes campos de los artÃ­culos:

* nombre_articulo (str): Nombre del producto o artÃ­culo.
* precio (int): Precio del producto en COP.
* calificacion_promedio (float): ValoraciÃ³n promedio del producto.
* antidad_calificaciones (int): NÃºmero de valoraciones que ha recibido.
* descripcion (str): DescripciÃ³n textual del producto.
* enlace_articulo (str): URL Ãºnica del artÃ­culo.

Con estos campos extraidos puede realizarse un posterior anÃ¡lisis de mercado, entender cuales son los precios mÃ¡s competitivos, analizar quÃ© productos representan un mejor beneficio en tÃ©rminos de calidad/precio contrastando las variables de calificaciones con la del precio, monitoreo de catÃ¡logos, etc.


---

## ğŸ¤ Stack TecnolÃ³gico y JustificaciÃ³n

| TecnologÃ­a              | Rol                       | JustificaciÃ³n                                                                |
| ----------------------- | ------------------------- | ---------------------------------------------------------------------------- |
| **Python 3.10+**        | Lenguaje principal        | Sencillo, robusto y con librerÃ­as maduras.                                   |
| **FastAPI**             | Backend REST API          | RÃ¡pido, moderno y con validaciÃ³n automÃ¡tica (Pydantic).                      |
| **PostgreSQL (Render)** | Base de datos en la nube  | Robusta, gratuita en Render, soporte JSON.                                   |
| **SQLAlchemy**          | ORM                       | Permite manejar la base de datos como objetos, con flexibilidad y seguridad. |
| **requests / aiohttp**  | HTTP sync/async           | Para scraping y concurrencia.                                                |
| **BeautifulSoup**       | Parser HTML               | Limpieza de contenido web.                                                   |
| **pandas**              | Manejo de datos tabulares | Escritura de CSVs y depuraciÃ³n.                                              |
| **argparse**            | CLI                       | ParametrizaciÃ³n del script.                                                  |

---

## ğŸš€ Estructura del Proyecto

```bash
mercadolibre_scraper_backend/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ db/           # ConexiÃ³n y esquema de base de datos
â”‚   â”‚   â”œâ”€â”€ routers/      # Endpoints FastAPI
â”‚   â”‚   â””â”€â”€ main.py      # InicializaciÃ³n del servidor
â”œâ”€â”€ scraping/              # LÃ³gica del scraper
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ automation.py   # Script CLI para ejecutar todo el flujo
â”œâ”€â”€ backups/               # CSVs generados por scraping
â”œâ”€â”€ logs/                  # Registros de ejecuciÃ³n
â”œâ”€â”€ .env                   # Variables de entorno sensibles
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â””â”€â”€ README.md
```

---

## ğŸ” Flujo General del Proyecto

1. **Scraper** busca productos segÃºn un tÃ©rmino de bÃºsqueda y hace scraping asincrÃ³nico.
2. **Limpieza de datos** normaliza precios, enlaces, descripciones y calificaciones.
3. **VerificaciÃ³n incremental**: se consultan los enlaces ya almacenados en la base de datos para evitar duplicados.
4. **Carga a la API REST**: los registros nuevos se envÃ­an al backend en FastAPI.
5. **Backup**: se guarda un CSV local con los resultados scrapeados.
6. **Logs**: se registran errores, resumen del proceso y timestamp.

---

## ğŸ—ï¸ Diagrama de Arquitectura

```
           +----------------------+     +------------------+
           |   Script CLI (user)  | --> | automation.py     |
           +----------------------+     +--------+---------+
                                                 |
                                                 |
                                         +-------v--------+
                                         | scraping/      |
                                         | scraper.py     |
                                         +-------+--------+
                                                 |
                                       +---------v----------+
                                       | BeautifulSoup +    |
                                       | Aiohttp/Requests   |
                                       +---------+----------+
                                                 |
                                         +-------v--------+
                                         | Limpieza CSV    |
                                         +-------+--------+
                                                 |
                                  +--------------v-------------+
                                  |  FastAPI Backend            |
                                  |  Routers + Schemas         |
                                  +--------------+-------------+
                                                 |
                                  +--------------v-------------+
                                  |   SQLAlchemy ORM Models     |
                                  +--------------+-------------+
                                                 |
                                         +-------v--------+
                                         | PostgreSQL DB  |
                                         | Render Cloud   |
                                         +----------------+
```

---

## ğŸ“ƒ Tabla de Funciones Clave

| Archivo         | FunciÃ³n                           | PropÃ³sito                                                     |
| --------------- | --------------------------------- | ------------------------------------------------------------- |
| `scraper.py`    | `obtener_url_todos_los_articulos` | PaginaciÃ³n de resultados y recolecciÃ³n de URLs de productos.  |
|                 | `scrapear_lista_articulos_async`  | Scraping asincrÃ³nico de cada producto.                        |
|                 | `limpiar_datos_articulos`         | NormalizaciÃ³n de precios, enlaces, y validaciÃ³n de registros. |
|                 | `guardar_en_csv`                  | Almacenamiento local con timestamp.                           |
| `automation.py` | `main(args)`                      | Orquesta scraping + limpieza + backup + carga.                |
|                 | `obtener_enlaces_existentes`      | Recupera enlaces desde la API con paginaciÃ³n.                 |
|                 | `enviar_registro`                 | POST a la API con manejo de errores y duplicados.             |

---

## âš–ï¸ Despliegue y EjecuciÃ³n

### 1. Crear entorno virtual e instalar dependencias

```bash
python -m venv venv
venv\Scripts\activate   # (Windows)
pip install -r requirements.txt
```

### 2. Configurar las variables en `.env`

```env
DB_HOST=your-db-host.render.com
DB_NAME=your-db-name
DB_USER=your-db-user
DB_PASSWORD=your-db-password
DB_PORT=your-db-port
```

### 3. Iniciar la API (desde la carpeta `backend`)

```bash
uvicorn app.main:app --reload
```

### 4. Ejecutar el scraper con CLI o cron

Con CLI:
```bash
python scripts/automation.py --articulo "laptop hp" --paginas 3 --guardar_csv --concurrencia 50
```

Con Cron (Por ejemplo para ejecutar todos los dÃ­as a las 09:00 AM):
```bash
crontab -e

0 9 * * * /usr/bin/python3 /ruta/completa/al/proyecto/scripts/automation.py --articulo "laptop hp" --paginas 3 --guardar_csv --concurrencia 50 >> /ruta/completa/al/proyecto/logs/cron.log 2>&1
```

---

## ğŸ¥ Video de DemostraciÃ³n

\[Enlace al video en Google Drive o YouTube]

---

## ğŸ”– Recomendaciones Finales

* El sistema estÃ¡ preparado para correr diariamente como tarea programada (cron).
* Se pueden ampliar los campos de scraping segÃºn la categorÃ­a del producto.
* Ideal para monitorear precios, disponibilidad o cambios en catÃ¡logos.

---


